# -*- coding: utf-8 -*-
"""xception_net.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQ_1uouuhq_OtGW0xEdcloAka7NkoUP0
"""

"""
Xception Network Implementation from Scratch using Keras 3
for ISAR Dataset Classification

Reference: Xception: Deep Learning with Depthwise Separable Convolutions
https://arxiv.org/abs/1610.02357
"""

import os
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import tensorflow as tf
import keras
from keras import layers, models, optimizers, callbacks
from keras.utils import image_dataset_from_directory
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Set random seeds for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

class XceptionFromScratch:
    """Xception architecture implemented from scratch using Keras 3"""

    def __init__(self, include_top=True, input_shape=(299, 299, 3), classes=1000, pooling=None):
        self.include_top = include_top
        self.input_shape = input_shape
        self.classes = classes
        self.pooling = pooling

    def separable_conv_block(self, x, filters, prefix, stride=1):
        """Separable convolution block with batch normalization and ReLU"""
        x = layers.SeparableConv2D(
            filters,
            (3, 3),
            padding='same',
            use_bias=False,
            strides=stride,
            name=f'{prefix}_sepconv'
        )(x)
        x = layers.BatchNormalization(name=f'{prefix}_sepconv_bn')(x)
        x = layers.Activation('relu', name=f'{prefix}_sepconv_act')(x)
        return x

    def conv_block(self, x, filters, kernel_size=(3, 3), stride=1, name=None):
        """Standard convolution block with batch normalization and ReLU"""
        x = layers.Conv2D(
            filters,
            kernel_size,
            strides=stride,
            padding='same' if stride == 1 else 'valid',
            use_bias=False,
            name=name
        )(x)
        x = layers.BatchNormalization(name=f'{name}_bn')(x)
        x = layers.Activation('relu', name=f'{name}_act')(x)
        return x

    def entry_flow(self, img_input):
        """Entry flow of Xception network"""
        # Block 1
        x = layers.Conv2D(32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1')(img_input)
        x = layers.BatchNormalization(name='block1_conv1_bn')(x)
        x = layers.Activation('relu', name='block1_conv1_act')(x)

        x = layers.Conv2D(64, (3, 3), use_bias=False, name='block1_conv2')(x)
        x = layers.BatchNormalization(name='block1_conv2_bn')(x)
        x = layers.Activation('relu', name='block1_conv2_act')(x)

        # Block 2
        residual = layers.Conv2D(128, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
        residual = layers.BatchNormalization()(residual)

        x = layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)
        x = layers.BatchNormalization(name='block2_sepconv1_bn')(x)
        x = layers.Activation('relu', name='block2_sepconv2_act')(x)
        x = layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)
        x = layers.BatchNormalization(name='block2_sepconv2_bn')(x)

        x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)
        x = layers.add([x, residual])

        # Block 3
        residual = layers.Conv2D(256, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
        residual = layers.BatchNormalization()(residual)

        x = layers.Activation('relu', name='block3_sepconv1_act')(x)
        x = layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)
        x = layers.BatchNormalization(name='block3_sepconv1_bn')(x)
        x = layers.Activation('relu', name='block3_sepconv2_act')(x)
        x = layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)
        x = layers.BatchNormalization(name='block3_sepconv2_bn')(x)

        x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)
        x = layers.add([x, residual])

        # Block 4
        residual = layers.Conv2D(728, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
        residual = layers.BatchNormalization()(residual)

        x = layers.Activation('relu', name='block4_sepconv1_act')(x)
        x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)
        x = layers.BatchNormalization(name='block4_sepconv1_bn')(x)
        x = layers.Activation('relu', name='block4_sepconv2_act')(x)
        x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)
        x = layers.BatchNormalization(name='block4_sepconv2_bn')(x)

        x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)
        x = layers.add([x, residual])

        return x

    def middle_flow(self, x):
        """Middle flow with 8 identical blocks"""
        for i in range(8):
            residual = x
            prefix = f'block{i + 5}'

            x = layers.Activation('relu', name=f'{prefix}_sepconv1_act')(x)
            x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=f'{prefix}_sepconv1')(x)
            x = layers.BatchNormalization(name=f'{prefix}_sepconv1_bn')(x)

            x = layers.Activation('relu', name=f'{prefix}_sepconv2_act')(x)
            x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=f'{prefix}_sepconv2')(x)
            x = layers.BatchNormalization(name=f'{prefix}_sepconv2_bn')(x)

            x = layers.Activation('relu', name=f'{prefix}_sepconv3_act')(x)
            x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=f'{prefix}_sepconv3')(x)
            x = layers.BatchNormalization(name=f'{prefix}_sepconv3_bn')(x)

            x = layers.add([x, residual])

        return x

    def exit_flow(self, x):
        """Exit flow of Xception network"""
        # Block 13
        residual = layers.Conv2D(1024, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
        residual = layers.BatchNormalization()(residual)

        x = layers.Activation('relu', name='block13_sepconv1_act')(x)
        x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)
        x = layers.BatchNormalization(name='block13_sepconv1_bn')(x)
        x = layers.Activation('relu', name='block13_sepconv2_act')(x)
        x = layers.SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)
        x = layers.BatchNormalization(name='block13_sepconv2_bn')(x)

        x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)
        x = layers.add([x, residual])

        # Block 14
        x = layers.SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)
        x = layers.BatchNormalization(name='block14_sepconv1_bn')(x)
        x = layers.Activation('relu', name='block14_sepconv1_act')(x)

        x = layers.SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)
        x = layers.BatchNormalization(name='block14_sepconv2_bn')(x)
        x = layers.Activation('relu', name='block14_sepconv2_act')(x)

        return x

    def build_model(self):
        """Build the complete Xception model"""
        img_input = layers.Input(shape=self.input_shape)

        # Entry flow
        x = self.entry_flow(img_input)

        # Middle flow
        x = self.middle_flow(x)

        # Exit flow
        x = self.exit_flow(x)

        # Classification head
        if self.include_top:
            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)
            x = layers.Dense(self.classes, activation='softmax', name='predictions')(x)
        else:
            if self.pooling == 'avg':
                x = layers.GlobalAveragePooling2D()(x)
            elif self.pooling == 'max':
                x = layers.GlobalMaxPooling2D()(x)

        model = models.Model(img_input, x, name='xception')
        return model


class ISARClassifier:
    """ISAR Dataset Classifier using Xception"""

    def __init__(self, dataset_path, input_size=(299, 299), batch_size=32):
        self.dataset_path = Path(dataset_path)
        self.input_size = input_size
        self.batch_size = batch_size
        self.model = None
        self.history = None

    def load_dataset(self, validation_split=0.2, test_split=0.1):
        """Load ISAR dataset from directory structure"""
        print(f"Loading dataset from: {self.dataset_path}")

        # Create train dataset
        train_ds = image_dataset_from_directory(
            self.dataset_path,
            validation_split=validation_split + test_split,
            subset="training",
            seed=42,
            image_size=self.input_size,
            batch_size=self.batch_size
        )

        # Create validation + test dataset
        val_test_ds = image_dataset_from_directory(
            self.dataset_path,
            validation_split=validation_split + test_split,
            subset="validation",
            seed=42,
            image_size=self.input_size,
            batch_size=self.batch_size
        )

        # Split validation and test - Fixed the data type issue
        val_batches = tf.data.experimental.cardinality(val_test_ds).numpy()
        test_size = int(val_batches * test_split / (validation_split + test_split))

        test_ds = val_test_ds.take(test_size)
        val_ds = val_test_ds.skip(test_size)

        # Get class names
        self.class_names = train_ds.class_names
        self.num_classes = len(self.class_names)

        print(f"Found {self.num_classes} classes: {self.class_names}")
        print(f"Training batches: {tf.data.experimental.cardinality(train_ds).numpy()}")
        print(f"Validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}")
        print(f"Test batches: {tf.data.experimental.cardinality(test_ds).numpy()}")

        return train_ds, val_ds, test_ds

    def preprocess_dataset(self, dataset, is_training=False):
        """Preprocess dataset with normalization and augmentation"""
        # Normalization (Xception uses [-1, 1] range)
        normalization_layer = layers.Rescaling(1./127.5, offset=-1)

        if is_training:
            # Data augmentation for training
            data_augmentation = keras.Sequential([
                layers.RandomFlip("horizontal"),
                layers.RandomRotation(0.1),
                layers.RandomZoom(0.1),
                layers.RandomContrast(0.1),
            ])

            dataset = dataset.map(
                lambda x, y: (data_augmentation(x, training=True), y),
                num_parallel_calls=tf.data.AUTOTUNE
            )

        # Apply normalization
        dataset = dataset.map(
            lambda x, y: (normalization_layer(x), y),
            num_parallel_calls=tf.data.AUTOTUNE
        )

        return dataset.prefetch(tf.data.AUTOTUNE)

    def build_model(self):
        """Build Xception model for ISAR classification"""
        xception_builder = XceptionFromScratch(
            include_top=True,
            input_shape=(*self.input_size, 3),
            classes=self.num_classes
        )

        self.model = xception_builder.build_model()

        # Compile model
        self.model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        print(f"Model built successfully!")
        print(f"Total parameters: {self.model.count_params():,}")

        return self.model

    def train(self, train_ds, val_ds, epochs=50):
        """Train the model"""
        # Preprocess datasets
        train_ds = self.preprocess_dataset(train_ds, is_training=True)
        val_ds = self.preprocess_dataset(val_ds, is_training=False)

        # Callbacks
        callbacks_list = [
            callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7,
                verbose=1
            ),
            callbacks.ModelCheckpoint(
                'best_xception_isar.keras',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]

        print("Starting training...")
        self.history = self.model.fit(
            train_ds,
            epochs=epochs,
            validation_data=val_ds,
            callbacks=callbacks_list,
            verbose=1
        )

        return self.history

    def evaluate(self, test_ds):
        """Evaluate model on test dataset"""
        test_ds = self.preprocess_dataset(test_ds, is_training=False)

        print("Evaluating on test dataset...")
        test_loss, test_accuracy = self.model.evaluate(test_ds, verbose=1)
        print(f"Test accuracy: {test_accuracy:.4f}")
        print(f"Test loss: {test_loss:.4f}")

        return test_loss, test_accuracy

    def plot_training_history(self):
        """Plot training history"""
        if self.history is None:
            print("No training history available!")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # Plot accuracy
        ax1.plot(self.history.history['accuracy'], label='Training Accuracy')
        ax1.plot(self.history.history['val_accuracy'], label='Validation Accuracy')
        ax1.set_title('Model Accuracy')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()
        ax1.grid(True)

        # Plot loss
        ax2.plot(self.history.history['loss'], label='Training Loss')
        ax2.plot(self.history.history['val_loss'], label='Validation Loss')
        ax2.set_title('Model Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

    def predict_and_visualize(self, test_ds, num_samples=9):
        """Make predictions and visualize results"""
        test_ds = self.preprocess_dataset(test_ds, is_training=False)

        # Get a batch of test images
        for images, labels in test_ds.take(1):
            predictions = self.model.predict(images)
            predicted_classes = np.argmax(predictions, axis=1)

            plt.figure(figsize=(15, 10))
            for i in range(min(num_samples, len(images))):
                plt.subplot(3, 3, i + 1)
                # Denormalize image for display
                img = (images[i] + 1) * 127.5
                img = tf.cast(img, tf.uint8)
                plt.imshow(img)

                true_class = self.class_names[labels[i]]
                pred_class = self.class_names[predicted_classes[i]]
                confidence = np.max(predictions[i])

                color = 'green' if true_class == pred_class else 'red'
                plt.title(f'True: {true_class}\nPred: {pred_class}\nConf: {confidence:.2f}',
                         color=color)
                plt.axis('off')

            plt.tight_layout()
            plt.savefig('predictions_visualization.png', dpi=300, bbox_inches='tight')
            plt.show()
            break


def main():
    """Main function to run ISAR classification with Xception"""
    # Dataset path - Updated for Google Colab
    dataset_path = "/content/drive/MyDrive/concave"

    # Initialize classifier
    classifier = ISARClassifier(
        dataset_path=dataset_path,
        input_size=(299, 299),  # Xception standard input size
        batch_size=16  # Adjust based on your GPU memory
    )

    try:
        # Load dataset
        train_ds, val_ds, test_ds = classifier.load_dataset()

        # Build model
        model = classifier.build_model()

        # Print model summary
        print("\nModel Architecture:")
        model.summary()

        # Train model
        history = classifier.train(train_ds, val_ds, epochs=50)

        # Plot training history
        classifier.plot_training_history()

        # Evaluate on test set
        test_loss, test_accuracy = classifier.evaluate(test_ds)

        # Visualize predictions
        classifier.predict_and_visualize(test_ds)

        print(f"\nFinal Results:")
        print(f"Test Accuracy: {test_accuracy:.4f}")
        print(f"Test Loss: {test_loss:.4f}")

    except Exception as e:
        print(f"Error occurred: {str(e)}")
        print("Please check your dataset path and structure.")
        print("Expected structure:")
        print("/content/drive/MyDrive/concave/")
        print("  ├── class1/")
        print("  │   ├── image1.jpg")
        print("  │   └── image2.jpg")
        print("  ├── class2/")
        print("  │   ├── image1.jpg")
        print("  │   └── image2.jpg")
        print("  └── ...")


if __name__ == "__main__":
    main()